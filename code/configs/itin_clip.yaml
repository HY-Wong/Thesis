model:
  visual:
    model: "clip"
    train_baseline: false
    finetune: false
    embedding_dim: 512
  text:
    model: "clip"
    train_baseline: false
    max_len: 100
    num_layers: 2
    embedding_dim: 512
    dropout: 0.25
  region:
    model: "fasterrcnn_bua"
    num_regions: 2
    embedding_dim: 2048
  without_alignment: false
  without_gating: false
  cross_modal_baseline: false
  context_baseline: false
  hidden_dim1: 512
  hidden_dim2: 512
  embedding_dim: 512
  dropout: 0.25
  _lambda: 0.2
  num_attns: 4
  num_classes: 3

training:
  batch_size_mvsa_single: 64
  batch_size_mvsa_multiple: 128
  batch_size_climate_tv: 64
  epochs: 10
  optimizer: "adam_w" # "adam", "adam_w"
  lr: 0.001
  weight_decay: 0.0001
  lr_decay_factor: 0.1
  lr_decay_step_size: 3
  grad_clip: 1.0
  use_class_weight: false
  gamma: 0.0